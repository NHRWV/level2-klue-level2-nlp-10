{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, Trainer, TrainingArguments, RobertaConfig, RobertaTokenizer, RobertaForSequenceClassification, BertTokenizer\n",
    "from load_data import *\n",
    "import wandb\n",
    "from loss import *\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministric = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# BiLSTM\n",
    "class Model_BiLSTM(nn.Module):\n",
    "  def __init__(self, MODEL_NAME):\n",
    "    super().__init__()\n",
    "    self.model_config =  AutoConfig.from_pretrained(MODEL_NAME)\n",
    "    self.model_config.num_labels = 30\n",
    "    self.model = AutoModel.from_pretrained(MODEL_NAME, config = self.model_config)\n",
    "    self.hidden_dim = self.model_config.hidden_size\n",
    "    self.lstm= nn.LSTM(input_size= self.hidden_dim, hidden_size= self.hidden_dim, num_layers= 1, batch_first= True, bidirectional= True)\n",
    "    self.fc = nn.Linear(self.hidden_dim * 2, self.model_config.num_labels)\n",
    "  \n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    output = self.model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state # (batch, max_len, hidden_dim)\n",
    "\n",
    "    hidden, (last_hidden, last_cell) = self.lstm(output)\n",
    "    output = torch.cat((last_hidden[0], last_hidden[1]), dim=1)\n",
    "    # hidden : (batch, max_len, hidden_dim * 2)\n",
    "    # last_hidden : (2, batch, hidden_dim)\n",
    "    # last_cell : (2, batch, hidden_dim)\n",
    "    # output : (batch, hidden_dim * 2)\n",
    "\n",
    "    logits = self.fc(output) # (batch, num_labels)\n",
    "\n",
    "    return {'logits' : logits}\n",
    "\n",
    "# BiGRU\n",
    "class Model_BiGRU(nn.Module):\n",
    "  def __init__(self, MODEL_NAME):\n",
    "    super().__init__()\n",
    "    self.model_config =  AutoConfig.from_pretrained(MODEL_NAME)\n",
    "    self.model_config.num_labels = 30\n",
    "    self.model = AutoModel.from_pretrained(MODEL_NAME, config = self.model_config)\n",
    "    self.hidden_dim = self.model_config.hidden_size\n",
    "    self.gru= nn.GRU(input_size= self.hidden_dim, hidden_size= self.hidden_dim, num_layers= 1, batch_first= True, bidirectional= True)\n",
    "    self.fc = nn.Linear(self.hidden_dim * 2, self.model_config.num_labels)\n",
    "  \n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    output = self.model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "    # (batch, max_len, hidden_dim)\n",
    "\n",
    "    hidden, last_hidden = self.gru(output)\n",
    "    output = torch.cat((last_hidden[0], last_hidden[1]), dim=1)\n",
    "    # hidden : (batch, max_len, hidden_dim * 2)\n",
    "    # last_hidden : (2, batch, hidden_dim)\n",
    "    # output : (batch, hidden_dim * 2)\n",
    "\n",
    "    logits = self.fc(output)\n",
    "    # logits : (batch, num_labels)\n",
    "\n",
    "    return {'logits' : logits}\n",
    "\n",
    "# FC\n",
    "class Model_FC(nn.Module):\n",
    "  def __init__(self, MODEL_NAME):\n",
    "    super().__init__()\n",
    "    self.model_config =  AutoConfig.from_pretrained(MODEL_NAME)\n",
    "    self.model_config.num_labels = 30\n",
    "    self.model = AutoModel.from_pretrained(MODEL_NAME, config = self.model_config)\n",
    "    self.hidden_dim = self.model_config.hidden_size\n",
    "    self.fc = nn.Linear(self.hidden_dim * 128, self.model_config.num_labels)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.softmax = nn.Softmax(dim=1)\n",
    "  \n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    output = self.model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state # (batch, max_len, hidden_dim)\n",
    "    output = output.view(output.shape[0], -1) # (batch, max_len * hidden_dim)\n",
    "    output = self.fc(output) # (batch, num_labels)\n",
    "    output = self.relu(output)\n",
    "    logits = self.softmax(output)\n",
    "    \n",
    "    return {'logits' : logits}\n",
    "\n",
    "class Custom_Trainer(Trainer) :\n",
    "    def __init__(self,loss_name, *args, **kwargs) :\n",
    "        super().__init__(*args,**kwargs)\n",
    "        self.loss_name = loss_name\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs = False) :\n",
    "        labels = inputs.pop('labels')\n",
    "        outputs = model(**inputs)\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu:0')\n",
    "\n",
    "        # 인덱스에 맞춰서 과거 ouput을 다 저장\n",
    "        if self.args.past_index >=0:\n",
    "            self._past= outputs[self.args.past_index]\n",
    "\n",
    "        if self.loss_name == 'CrossEntropyLoss':\n",
    "            custom_loss= torch.nn.CrossEntropyLoss().to(device)\n",
    "            loss= custom_loss(outputs['logits'], labels)\n",
    "        \n",
    "        elif self.loss_name == 'FOCAL_LOSS':\n",
    "            custom_loss = FocalLoss(gamma = 1).to(device)\n",
    "            loss = custom_loss(outputs['logits'],labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "def klue_re_micro_f1(preds, labels):\n",
    "    \"\"\"KLUE-RE micro f1 (except no_relation)\"\"\"\n",
    "    label_list = ['no_relation', 'org:top_members/employees', 'org:members',\n",
    "       'org:product', 'per:title', 'org:alternate_names',\n",
    "       'per:employee_of', 'org:place_of_headquarters', 'per:product',\n",
    "       'org:number_of_employees/members', 'per:children',\n",
    "       'per:place_of_residence', 'per:alternate_names',\n",
    "       'per:other_family', 'per:colleagues', 'per:origin', 'per:siblings',\n",
    "       'per:spouse', 'org:founded', 'org:political/religious_affiliation',\n",
    "       'org:member_of', 'per:parents', 'org:dissolved',\n",
    "       'per:schools_attended', 'per:date_of_death', 'per:date_of_birth',\n",
    "       'per:place_of_birth', 'per:place_of_death', 'org:founded_by',\n",
    "       'per:religion']\n",
    "    no_relation_label_idx = label_list.index(\"no_relation\")\n",
    "    label_indices = list(range(len(label_list)))\n",
    "    label_indices.remove(no_relation_label_idx)\n",
    "    return sklearn.metrics.f1_score(labels, preds, average=\"micro\", labels=label_indices) * 100.0\n",
    "\n",
    "def klue_re_auprc(probs, labels):\n",
    "    \"\"\"KLUE-RE AUPRC (with no_relation)\"\"\"\n",
    "    labels = np.eye(30)[labels]\n",
    "\n",
    "    score = np.zeros((30,))\n",
    "    for c in range(30):\n",
    "        targets_c = labels.take([c], axis=1).ravel()\n",
    "        preds_c = probs.take([c], axis=1).ravel()\n",
    "        precision, recall, _ = sklearn.metrics.precision_recall_curve(targets_c, preds_c)\n",
    "        score[c] = sklearn.metrics.auc(recall, precision)\n",
    "    return np.average(score) * 100.0\n",
    "\n",
    "def compute_metrics(pred):\n",
    "  \"\"\" validation을 위한 metrics function \"\"\"\n",
    "  labels = pred.label_ids\n",
    "  preds = pred.predictions.argmax(-1)\n",
    "  probs = pred.predictions\n",
    "\n",
    "  # calculate accuracy using sklearn's function\n",
    "  f1 = klue_re_micro_f1(preds, labels)\n",
    "  auprc = klue_re_auprc(probs, labels)\n",
    "  acc = accuracy_score(labels, preds) # 리더보드 평가에는 포함되지 않습니다.\n",
    "  wandb.log({'micro f1 score': f1})\n",
    "  return {\n",
    "      'micro f1 score': f1,\n",
    "      'auprc' : auprc,\n",
    "      'accuracy': acc,\n",
    "  }\n",
    "\n",
    "def label_to_num(label):\n",
    "  num_label = []\n",
    "  with open('dict_label_to_num.pkl', 'rb') as f:\n",
    "    dict_label_to_num = pickle.load(f)\n",
    "  for v in label:\n",
    "    num_label.append(dict_label_to_num[v])\n",
    "  \n",
    "  return num_label\n",
    "\n",
    "def train():\n",
    "  # load model and tokenizer\n",
    "  # MODEL_NAME = \"bert-base-uncased\"\n",
    "  #MODEL_NAME = \"monologg/koelectra-base-v3-discriminator\"\n",
    "  MODEL_NAME = \"klue/roberta-large\"\n",
    "  tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "  # load dataset\n",
    "  train_dataset = load_data(\"/opt/ml/dataset/train/train.csv\")\n",
    "  # dev_dataset = load_data(\"../dataset/train/dev.csv\") # validation용 데이터는 따로 만드셔야 합니다.\n",
    "\n",
    "  train_label = label_to_num(train_dataset['label'].values)\n",
    "  # dev_label = label_to_num(dev_dataset['label'].values)\n",
    "\n",
    "  # tokenizing dataset\n",
    "  tokenized_train = TEMP_tokenized_dataset(train_dataset, tokenizer)\n",
    "  # tokenized_dev = tokenized_dataset(dev_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id                                           sentence  \\\n",
      "0          0  〈Something〉는 조지 해리슨이 쓰고 비틀즈가 1969년 앨범 《Abbey R...   \n",
      "1          1  호남이 기반인 바른미래당·대안신당·민주평화당이 우여곡절 끝에 합당해 민생당(가칭)으...   \n",
      "2          2  K리그2에서 성적 1위를 달리고 있는 광주FC는 지난 26일 한국프로축구연맹으로부터...   \n",
      "3          3  균일가 생활용품점 (주)아성다이소(대표 박정부)는 코로나19 바이러스로 어려움을 겪...   \n",
      "4          4  1967년 프로 야구 드래프트 1순위로 요미우리 자이언츠에게 입단하면서 등번호는 8...   \n",
      "...      ...                                                ...   \n",
      "32465  32465  한국당은 7일 오전 9시부터 오후 5시까지 진행된 원내대표 및 정책위의장 후보자 등...   \n",
      "32466  32466  법포는 다시 최시형, 서병학, 손병희 직계인 북접과 다시 서장옥, 전봉준, 김개남을...   \n",
      "32467  32467  완도군(군수 신우철)이 국토교통부에서 실시한 '2019 교통문화지수 실태조사'에서 ...   \n",
      "32468  32468  중앙일보, JTBC 회장을 지낸 이후 중앙홀딩스 회장, 재단법인 한반도평화만들기 이...   \n",
      "32469  32469  화순군(군수 구충곤)은 17일 동면의 이장 20여 명이 코로나 19 예방을 위해 버...   \n",
      "\n",
      "              subject_entity         object_entity                      label  \n",
      "0            @ + ORG + 비틀즈 @    # ^ PER ^ 조지 해리슨 #                no_relation  \n",
      "1          @ + ORG + 민주평화당 @      # ^ ORG ^ 대안신당 #                no_relation  \n",
      "2           @ + ORG + 광주FC @  # ^ ORG ^ 한국프로축구연맹 #              org:member_of  \n",
      "3          @ + ORG + 아성다이소 @       # ^ PER ^ 박정부 #  org:top_members/employees  \n",
      "4      @ + ORG + 요미우리 자이언츠 @      # ^ DAT ^ 1967 #                no_relation  \n",
      "...                      ...                   ...                        ...  \n",
      "32465        @ + PER + 유기준 @  # ^ LOC ^ 부산 서구·동구 #            per:employee_of  \n",
      "32466        @ + PER + 최시형 @       # ^ PER ^ 손병희 #             per:colleagues  \n",
      "32467        @ + ORG + 완도군 @       # ^ PER ^ 신우철 #  org:top_members/employees  \n",
      "32468       @ + ORG + JTBC @     # ^ ORG ^ 중앙홀딩스 #                no_relation  \n",
      "32469        @ + ORG + 화순군 @       # ^ PER ^ 구충곤 #  org:top_members/employees  \n",
      "\n",
      "[32470 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"klue/roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "train_dataset = load_data(\"/opt/ml/dataset/train/train.csv\")\n",
    "train_label = label_to_num(train_dataset['label'].values)\n",
    "tokenized_train = TEMP_tokenized_dataset(train_dataset, tokenizer)\n",
    "\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, Trainer, TrainingArguments, RobertaConfig, RobertaTokenizer, RobertaForSequenceClassification, BertTokenizer,EarlyStoppingCallback\n",
    "from load_data import *\n",
    "import wandb\n",
    "from loss import *\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministric = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# BiLSTM\n",
    "class Model_BiLSTM(nn.Module):\n",
    "  def __init__(self, MODEL_NAME):\n",
    "    super().__init__()\n",
    "    self.model_config =  AutoConfig.from_pretrained(MODEL_NAME)\n",
    "    self.model_config.num_labels = 30\n",
    "    self.model = AutoModel.from_pretrained(MODEL_NAME, config = self.model_config)\n",
    "    self.hidden_dim = self.model_config.hidden_size\n",
    "    self.lstm= nn.LSTM(input_size= self.hidden_dim, hidden_size= self.hidden_dim, num_layers= 1, batch_first= True, bidirectional= True)\n",
    "    self.fc = nn.Linear(self.hidden_dim * 2, self.model_config.num_labels)\n",
    "  \n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    output = self.model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state # (batch, max_len, hidden_dim)\n",
    "\n",
    "    hidden, (last_hidden, last_cell) = self.lstm(output)\n",
    "    output = torch.cat((last_hidden[0], last_hidden[1]), dim=1)\n",
    "    # hidden : (batch, max_len, hidden_dim * 2)\n",
    "    # last_hidden : (2, batch, hidden_dim)\n",
    "    # last_cell : (2, batch, hidden_dim)\n",
    "    # output : (batch, hidden_dim * 2)\n",
    "\n",
    "    logits = self.fc(output) # (batch, num_labels)\n",
    "\n",
    "    return {'logits' : logits}\n",
    "\n",
    "# BiGRU\n",
    "class Model_BiGRU(nn.Module):\n",
    "  def __init__(self, MODEL_NAME):\n",
    "    super().__init__()\n",
    "    self.model_config =  AutoConfig.from_pretrained(MODEL_NAME)\n",
    "    self.model_config.num_labels = 30\n",
    "    self.model = AutoModel.from_pretrained(MODEL_NAME, config = self.model_config)\n",
    "    self.hidden_dim = self.model_config.hidden_size\n",
    "    self.gru= nn.GRU(input_size= self.hidden_dim, hidden_size= self.hidden_dim, num_layers= 1, batch_first= True, bidirectional= True)\n",
    "    self.fc = nn.Linear(self.hidden_dim * 2, self.model_config.num_labels)\n",
    "  \n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    output = self.model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "    # (batch, max_len, hidden_dim)\n",
    "\n",
    "    hidden, last_hidden = self.gru(output)\n",
    "    output = torch.cat((last_hidden[0], last_hidden[1]), dim=1)\n",
    "    # hidden : (batch, max_len, hidden_dim * 2)\n",
    "    # last_hidden : (2, batch, hidden_dim)\n",
    "    # output : (batch, hidden_dim * 2)\n",
    "\n",
    "    logits = self.fc(output)\n",
    "    # logits : (batch, num_labels)\n",
    "\n",
    "    return {'logits' : logits}\n",
    "\n",
    "# FC\n",
    "class Model_FC(nn.Module):\n",
    "  def __init__(self, MODEL_NAME):\n",
    "    super().__init__()\n",
    "    self.model_config =  AutoConfig.from_pretrained(MODEL_NAME)\n",
    "    self.model_config.num_labels = 30\n",
    "    self.model = AutoModel.from_pretrained(MODEL_NAME, config = self.model_config)\n",
    "    self.hidden_dim = self.model_config.hidden_size\n",
    "    self.fc = nn.Linear(self.hidden_dim * 128, self.model_config.num_labels)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.softmax = nn.Softmax(dim=1)\n",
    "  \n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    output = self.model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state # (batch, max_len, hidden_dim)\n",
    "    output = output.view(output.shape[0], -1) # (batch, max_len * hidden_dim)\n",
    "    output = self.fc(output) # (batch, num_labels)\n",
    "    output = self.relu(output)\n",
    "    logits = self.softmax(output)\n",
    "    \n",
    "    return {'logits' : logits}\n",
    "\n",
    "class Custom_Trainer(Trainer) :\n",
    "    def __init__(self,loss_name, *args, **kwargs) :\n",
    "        super().__init__(*args,**kwargs)\n",
    "        self.loss_name = loss_name\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs = False) :\n",
    "        labels = inputs.pop('labels')\n",
    "        outputs = model(**inputs)\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu:0')\n",
    "\n",
    "        # 인덱스에 맞춰서 과거 ouput을 다 저장\n",
    "        if self.args.past_index >=0:\n",
    "            self._past= outputs[self.args.past_index]\n",
    "\n",
    "        if self.loss_name == 'CrossEntropyLoss':\n",
    "            custom_loss= torch.nn.CrossEntropyLoss().to(device)\n",
    "            loss= custom_loss(outputs['logits'], labels)\n",
    "        \n",
    "        elif self.loss_name == 'FOCAL_LOSS':\n",
    "            custom_loss = FocalLoss(gamma = 1).to(device)\n",
    "            loss = custom_loss(outputs['logits'],labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "def klue_re_micro_f1(preds, labels):\n",
    "    \"\"\"KLUE-RE micro f1 (except no_relation)\"\"\"\n",
    "    label_list = ['no_relation', 'org:top_members/employees', 'org:members',\n",
    "       'org:product', 'per:title', 'org:alternate_names',\n",
    "       'per:employee_of', 'org:place_of_headquarters', 'per:product',\n",
    "       'org:number_of_employees/members', 'per:children',\n",
    "       'per:place_of_residence', 'per:alternate_names',\n",
    "       'per:other_family', 'per:colleagues', 'per:origin', 'per:siblings',\n",
    "       'per:spouse', 'org:founded', 'org:political/religious_affiliation',\n",
    "       'org:member_of', 'per:parents', 'org:dissolved',\n",
    "       'per:schools_attended', 'per:date_of_death', 'per:date_of_birth',\n",
    "       'per:place_of_birth', 'per:place_of_death', 'org:founded_by',\n",
    "       'per:religion']\n",
    "    no_relation_label_idx = label_list.index(\"no_relation\")\n",
    "    label_indices = list(range(len(label_list)))\n",
    "    label_indices.remove(no_relation_label_idx)\n",
    "    return sklearn.metrics.f1_score(labels, preds, average=\"micro\", labels=label_indices) * 100.0\n",
    "\n",
    "def klue_re_auprc(probs, labels):\n",
    "    \"\"\"KLUE-RE AUPRC (with no_relation)\"\"\"\n",
    "    labels = np.eye(30)[labels]\n",
    "\n",
    "    score = np.zeros((30,))\n",
    "    for c in range(30):\n",
    "        targets_c = labels.take([c], axis=1).ravel()\n",
    "        preds_c = probs.take([c], axis=1).ravel()\n",
    "        precision, recall, _ = sklearn.metrics.precision_recall_curve(targets_c, preds_c)\n",
    "        score[c] = sklearn.metrics.auc(recall, precision)\n",
    "    return np.average(score) * 100.0\n",
    "\n",
    "def compute_metrics(pred):\n",
    "  \"\"\" validation을 위한 metrics function \"\"\"\n",
    "  labels = pred.label_ids\n",
    "  preds = pred.predictions.argmax(-1)\n",
    "  probs = pred.predictions\n",
    "\n",
    "  # calculate accuracy using sklearn's function\n",
    "  f1 = klue_re_micro_f1(preds, labels)\n",
    "  auprc = klue_re_auprc(probs, labels)\n",
    "  acc = accuracy_score(labels, preds) # 리더보드 평가에는 포함되지 않습니다.\n",
    "  wandb.log({'micro f1 score': f1})\n",
    "  return {\n",
    "      'micro f1 score': f1,\n",
    "      'auprc' : auprc,\n",
    "      'accuracy': acc,\n",
    "  }\n",
    "\n",
    "def label_to_num(label):\n",
    "  num_label = []\n",
    "  with open('dict_label_to_num.pkl', 'rb') as f:\n",
    "    dict_label_to_num = pickle.load(f)\n",
    "  for v in label:\n",
    "    num_label.append(dict_label_to_num[v])\n",
    "  \n",
    "  return num_label\n",
    "\n",
    "def train():\n",
    "  # load model and tokenizer\n",
    "  # MODEL_NAME = \"bert-base-uncased\"\n",
    "  #MODEL_NAME = \"monologg/koelectra-base-v3-discriminator\"\n",
    "  MODEL_NAME = \"klue/roberta-large\"\n",
    "  tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "  # load dataset\n",
    "  train_dataset = load_data(\"/opt/ml/dataset/train/train.csv\")\n",
    "  # dev_dataset = load_data(\"../dataset/train/dev.csv\") # validation용 데이터는 따로 만드셔야 합니다.\n",
    "\n",
    "  # train_label = label_to_num(train_dataset['label'].values)\n",
    "  # dev_label = label_to_num(dev_dataset['label'].values)\n",
    "  split_dataset = train_dataset\n",
    "  split_label = train_dataset['label'].values\n",
    "\n",
    "  # tokenizing dataset\n",
    "  # tokenized_train = TEMP_tokenized_dataset(train_dataset, tokenizer)\n",
    "  # tokenized_dev = tokenized_dataset(dev_dataset, tokenizer)\n",
    "  kfold = StratifiedKFold(n_splits = 10, random_state = seed_everything(42))\n",
    "  for fold,(train_idx,val_idx) in enumerate(kfold.split(split_dataset, split_label)) :\n",
    "    wandb_run = wandb.init(project = 'huggingface', name = f'KFOLD_{fold}_TEM_with focal_loss')\n",
    "    print(\"-\"*20,f'fold: {fold} start',\"-\"*20)\n",
    "    train_dataset = split_dataset.iloc[train_idx]\n",
    "    val_dataset = split_dataset.iloc[val_idx]\n",
    "\n",
    "    train_label = label_to_num(train_dataset['label'].values)\n",
    "    val_label = label_to_num(val_dataset['label'].values)\n",
    "\n",
    "    tokenized_train, token_size= TEMP_tokenized_dataset(train_dataset, tokenizer)\n",
    "    tokenized_val,_= TEMP_tokenized_dataset(val_dataset, tokenizer)\n",
    "\n",
    "    trainset= RE_Dataset(tokenized_train, train_label)\n",
    "    valset= RE_Dataset(tokenized_val, val_label)\n",
    "\n",
    "    model =  Model_BiGRU(MODEL_NAME)\n",
    "    model.to(device)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',          # output directory\n",
    "        save_strategy = 'epoch',\n",
    "        save_total_limit=1,              # number of total save model.\n",
    "        save_steps=500,                 # model saving step.\n",
    "        num_train_epochs=5,              # total number of training epochs\n",
    "        learning_rate=3e-5,               # learning_rate\n",
    "        per_device_train_batch_size=32,  # batch size per device during training\n",
    "        per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "        # warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "        warmup_ratio = 0.1,\n",
    "        weight_decay=0.01,               # strength of weight decay\n",
    "        # label_smoothing_factor=0.1,\n",
    "        # lr_scheduler_type = 'constant_with_warmup',\n",
    "        logging_dir='./logs',            # directory for storing logs\n",
    "        logging_steps=100,              # log saving step.\n",
    "        evaluation_strategy='epoch', # evaluation strategy to adopt during training\n",
    "                                    # `no`: No evaluation during training.\n",
    "                                    # `steps`: Evaluate every `eval_steps`.\n",
    "                                    # `epoch`: Evaluate every end of epoch.\n",
    "        eval_steps = 500,            # evaluation step.\n",
    "        load_best_model_at_end = True,\n",
    "        report_to = 'wandb'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- fold: 0 start --------------------\n",
      "-------------------- fold: 1 start --------------------\n",
      "-------------------- fold: 2 start --------------------\n",
      "-------------------- fold: 3 start --------------------\n",
      "-------------------- fold: 4 start --------------------\n",
      "-------------------- fold: 5 start --------------------\n",
      "-------------------- fold: 6 start --------------------\n",
      "-------------------- fold: 7 start --------------------\n",
      "-------------------- fold: 8 start --------------------\n",
      "-------------------- fold: 9 start --------------------\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"klue/roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# load dataset\n",
    "train_dataset = load_data(\"/opt/ml/dataset/train/train.csv\")\n",
    "# dev_dataset = load_data(\"../dataset/train/dev.csv\") # validation용 데이터는 따로 만드셔야 합니다.\n",
    "\n",
    "# train_label = label_to_num(train_dataset['label'].values)\n",
    "# dev_label = label_to_num(dev_dataset['label'].values)\n",
    "split_dataset = train_dataset\n",
    "split_label = train_dataset['label'].values\n",
    "\n",
    "# tokenizing dataset\n",
    "# tokenized_train = TEMP_tokenized_dataset(train_dataset, tokenizer)\n",
    "# tokenized_dev = tokenized_dataset(dev_dataset, tokenizer)\n",
    "kfold = StratifiedKFold(n_splits = 10, random_state = seed_everything(42))\n",
    "for fold,(train_idx,val_idx) in enumerate(kfold.split(split_dataset, split_label)) :\n",
    "  #wandb_run = wandb.init(project = 'huggingface', name = f'KFOLD_{fold}_TEM_with focal_loss')\n",
    "  print(\"-\"*20,f'fold: {fold} start',\"-\"*20)\n",
    "  train_dataset = split_dataset.iloc[train_idx]\n",
    "  val_dataset = split_dataset.iloc[val_idx]\n",
    "\n",
    "  train_label = label_to_num(train_dataset['label'].values)\n",
    "  val_label = label_to_num(val_dataset['label'].values)\n",
    "\n",
    "  tokenized_train, token_size= TEMP_tokenized_dataset(train_dataset, tokenizer)\n",
    "  tokenized_val,_= TEMP_tokenized_dataset(val_dataset, tokenizer)\n",
    "\n",
    "  trainset= RE_Dataset(tokenized_train, train_label)\n",
    "  valset= RE_Dataset(tokenized_val, val_label)\n",
    "\n",
    "  # model =  Model_BiGRU(MODEL_NAME)\n",
    "  # model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train, token_size= TEMP_tokenized_dataset(train_dataset, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
